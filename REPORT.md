## Краткий отчёт по ноутбукам

| Файл | Тип модели | Основная идея | Ключевые параметры | Обучение | Выводы |
|------|------------|--------------|-------------------|----------|--------|
| **RNN_word.ipynb** | Word-level language model | Текст разбивается на **слова** → токены.<br>Embedding → 2-слойная (Bi-)GRU/LSTM → Dense(Vocab). | *VOCAB_SIZE* ≈ несколько тыс.<br>*EMBED_DIM* = 256<br>*HID_BI* = 150, *HID* = 100<br>*SEQ_LEN* ≈ 20–30<br>*epochs* = 20 | Учён 20 эпох; в конце демонстрируется генерация фраз по seed-строке. Кривые loss постепенно сходятся; переобучения нет. | Модель создаёт связные, но местами повторяющиеся предложения; словарный запас отражает тренировочный корпус. |
| **RNN_char.ipynb** | Char-level language model | Текст обрабатывается посимвольно. <br>One-hot/Embedding (size≈64-70) → многослойный GRU/LSTM. | *VOCAB_SIZE* (символов) ≈ 70<br>*EMBED_DIM* = 128<br>*RNN_UNITS* ≈ 256×2<br>*BATCH_SIZE* = 100<br>*epochs* = 20 | 20 эпох; loss падает медленнее, т.к. задачa сложнее. После обучения генерируются абзацы длиной 400–500 символов. | Генерируемый текст грамматически шумнее, но отражает орфографию и пунктуацию источника; модель успешна в «стиле» корпуса. |

## Общие моменты
- **Данные**: в обоих ноутбуках загружается один и тот же корпус (видимо, русскоязычный художественный текст).  
- **Подготовка**: очистка, токенизация, формирование пар «контекст → следующее слово/символ».  
- **Оптимизатор**: `Adam`, `sparse_categorical_crossentropy`.  
- **Генерация**: функция с температурой `temp` (0.5–1.0) для контроля креативности.  
- **Сохранение**: модели и токенизаторы сохраняются на диск для дальнейшего дообучения или инференса.

## Короткий вывод
- **Word-level RNN** лучше улавливает смысловые связи, но иногда выдаёт `UNK` или редкие слова; пригодна для генерации связных предложений.  
- **Char-level RNN** обучается дольше, но свободна от OOV-проблем; генерирует текст с более мелкой вариативностью и опечатками.  
- Для дальнейшего улучшения рекомендуются:  
  1. Более длинные контексты (BPTT 50–100).  
  2. Регуляризация (`Dropout`, `LayerNorm`).  
  3. Замена RNN на Transformer-Decoder (быстрее сходится и даёт лучшее качество).  
